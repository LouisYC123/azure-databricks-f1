{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48300110-9e91-430c-b0e9-aa4d4e14f12c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Ingest qualifying folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95af95d4-8b74-425e-9b89-018cca060812",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfbb1084-2594-4264-aeb7-2eb624f6b058",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 1 - Read the set of csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b11f29e2-b6a0-47f9-a379-71b9063bea49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qualifying_schema = StructType(fields=[\n",
    "    StructField(\"qualifyId\", IntegerType(), False ),\n",
    "    StructField(\"raceId\", IntegerType(), True ),\n",
    "    StructField(\"driverId\", IntegerType(), True ),\n",
    "    StructField(\"constructorId\", IntegerType(), True ),\n",
    "    StructField(\"number\", IntegerType(), True ),\n",
    "    StructField(\"position\", IntegerType(), True ),\n",
    "    StructField(\"q1\", StringType(), True ),\n",
    "    StructField(\"q2\", StringType(), True ),\n",
    "    StructField(\"q3\", StringType(), True ),\n",
    "])\n",
    "\n",
    "qualifying_df = spark.read.schema(qualifying_schema).option(\"multiline\", True).json(\"/mnt/formula1lgdl/raw/qualifying\") # specifying a folder here, not files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c584c12-77f7-4175-9fed-f8bfc32d9501",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 2 - Rename columns and add new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5987e091-9ba4-4325-bbe2-2a3f22100f64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = qualifying_df.withColumnRenamed(\"raceId\", \"race_id\") \\\n",
    ".withColumnRenamed(\"qualifyId\", \"qualify_id\") \\\n",
    ".withColumnRenamed(\"driverId\", \"driver_id\") \\\n",
    ".withColumnRenamed(\"constructorId\", \"constructor_id\") \\\n",
    ".withColumn(\"ingestion_date\", current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ef374f4-2895-48ea-8969-c22a446f939d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 3 - Write to output to processes container in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff819265-adbd-414e-a741-c6377b087324",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.write.mode(\"overwrite\").parquet(\"/mnt/formula1lgdl/processed/qualifying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0556920d-eead-4fd2-94cf-aeed81f81b77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_multiple_qualifying_files",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
