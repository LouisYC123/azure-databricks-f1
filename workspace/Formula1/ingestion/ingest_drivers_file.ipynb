{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4304078-f825-4066-a3da-08c89a59f888",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Ingest Drivers.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd0e97d-f87f-497c-8aba-9f2ff3b34285",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DataType\n",
    "from pyspark.sql.functions import col, concat, current_timestamp, lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e0892a7-6707-4276-bc24-3e00f6d46901",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 1 - Read the JSON file using the spark dataframe reader API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "255d0a7f-9f76-4188-ab76-007c5e5d49ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[20]: 'This is a nested json file so we will define the inner schema (name_schema) and the outer schema separately seperately (drivers_schema)'"
     ]
    }
   ],
   "source": [
    "\"\"\"This is a nested json file so we will define the inner schema (name_schema) and the outer schema separately seperately (drivers_schema)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a21227f-a1bf-43cb-9446-e92c3e6e0317",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "name_schema = StructType(fields=[\n",
    "    StructField(\"forename\", StringType(), True),\n",
    "    StructField(\"surname\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ba0780a-80d8-44d2-b2c5-d5b9397ace27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drivers_schema = StructType(fields=[\n",
    "    StructField(\"driverId\", IntegerType(), False),\n",
    "    StructField(\"driverRef\", StringType(), True),\n",
    "    StructField(\"number\", IntegerType(), True),\n",
    "    StructField(\"code\", StringType(), False),\n",
    "    StructField(\"name\", name_schema, False),\n",
    "    StructField(\"url\", StringType(), False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50345b62-72dc-44bf-8d96-824a81590b83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drivers_df = spark.read \\\n",
    ".schema(drivers_schema) \\\n",
    ".json(\"/mnt/formula1lgdl/raw/drivers.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f29a2a90-b294-4043-a36b-5191894e0d2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 2 - Rename columns and add new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3049eb5-c44a-4481-a544-3fbe34ff883a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drivers_with_columns_df = drivers_df.withColumnRenamed(\"driverId\", \"driver_id\") \\\n",
    ".withColumnRenamed(\"driverRef\", \"driver_ref\") \\\n",
    ".withColumn(\"ingestion_date\", current_timestamp()) \\\n",
    ".withColumn(\"name\", concat(col(\"name.forename\"), lit(\" \"), col(\"name.surname\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc5d039a-4670-4d09-87e7-1ee4ac0c93a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 3 - Drop the unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a09bec8-530d-4c82-b034-a573f9c7c8fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drivers_final_df = drivers_with_columns_df.drop(col(\"url\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1e715eb-3c59-4c0a-8aea-a283694801fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 4 - Write to output to processed container in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74ab6d13-bec0-4be8-bc2d-36217ef2b6af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drivers_final_df.write.mode(\"overwrite\").parquet(\"/mnt/formula1lgdl/processed/drivers\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_drivers_file",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
