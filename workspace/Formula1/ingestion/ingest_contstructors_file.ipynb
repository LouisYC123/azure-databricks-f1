{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3b9af67-2753-43bf-b7c3-b4346ef50d67",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Ingest constructors.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d97eabb4-8626-4cf1-aeea-67d4611ff570",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "196dbdf6-95ff-4820-a02e-dee884a749c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 1 - Read the JSON file using the spark dataframe reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba3eaf2f-baef-47d9-ac96-e0ebe2d37f3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DDL style of defining schemas\n",
    "constructor_schema = 'constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c53beb-a591-48ba-ac79-f8cb8db616ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "constructor_df = spark.read \\\n",
    ".schema(constructor_schema) \\\n",
    ".json(\"/mnt/formula1lgdl/raw/constructors.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a85b024-fb1f-4148-bed7-86420090b7a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Step 2 - Drop unwanted columns from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b81ad15f-8ef3-4185-a92c-42de25e0c187",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "constructor_df_dropped = constructor_df.drop(constructor_df['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c699af6c-625d-4d3f-9b04-f9144a1f7058",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 3 - Rename columns and add ingestion date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1037e1-8b19-4bc9-ad7c-49c52d403a7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "constructor_df_final = constructor_df_dropped.withColumnRenamed(\"constructorId\", \"constructor_id\") \\\n",
    ".withColumnRenamed(\"constructorRef\", \"constructor_ref\") \\\n",
    ".withColumn(\"ingestion_date\", current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "236a1b35-d641-48fe-968b-3f1251fb795e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 4 - Write output to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc07b2dd-a5ac-4526-851c-e76903a46e55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "constructor_df_final.write.mode(\"overwrite\").parquet(\"/mnt/formula1lgdl/processed/constructors\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2515800322316938,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_contstructors_file",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
